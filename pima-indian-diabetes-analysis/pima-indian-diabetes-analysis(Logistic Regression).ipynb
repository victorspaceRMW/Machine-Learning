{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Logistic Regression to Model the Pima-Indian Diabetes Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pima Indian tribe has the highest portion of diabetes patients in the world. What we will do in this project is to use logistic regression to model this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us read the dataset and import the necessary packages first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0    1   2   3    4     5      6   7  target\n",
      "0     6  148  72  35    0  33.6  0.627  50       1\n",
      "1     1   85  66  29    0  26.6  0.351  31       0\n",
      "2     8  183  64   0    0  23.3  0.672  32       1\n",
      "3     1   89  66  23   94  28.1  0.167  21       0\n",
      "4     0  137  40  35  168  43.1  2.288  33       1\n",
      "5     5  116  74   0    0  25.6  0.201  30       0\n",
      "6     3   78  50  32   88  31.0  0.248  26       1\n",
      "7    10  115   0   0    0  35.3  0.134  29       0\n",
      "8     2  197  70  45  543  30.5  0.158  53       1\n",
      "9     8  125  96   0    0   0.0  0.232  54       1\n",
      "10    4  110  92   0    0  37.6  0.191  30       0\n",
      "11   10  168  74   0    0  38.0  0.537  34       1\n",
      "12   10  139  80   0    0  27.1  1.441  57       0\n",
      "13    1  189  60  23  846  30.1  0.398  59       1\n",
      "14    5  166  72  19  175  25.8  0.587  51       1\n",
      "15    7  100   0   0    0  30.0  0.484  32       1\n",
      "16    0  118  84  47  230  45.8  0.551  31       1\n",
      "17    7  107  74   0    0  29.6  0.254  31       1\n",
      "18    1  103  30  38   83  43.3  0.183  33       0\n",
      "19    1  115  70  30   96  34.6  0.529  32       1\n",
      "20    3  126  88  41  235  39.3  0.704  27       0\n",
      "21    8   99  84   0    0  35.4  0.388  50       0\n",
      "22    7  196  90   0    0  39.8  0.451  41       1\n",
      "23    9  119  80  35    0  29.0  0.263  29       1\n",
      "24   11  143  94  33  146  36.6  0.254  51       1\n",
      "25   10  125  70  26  115  31.1  0.205  41       1\n",
      "26    7  147  76   0    0  39.4  0.257  43       1\n",
      "27    1   97  66  15  140  23.2  0.487  22       0\n",
      "28   13  145  82  19  110  22.2  0.245  57       0\n",
      "29    5  117  92   0    0  34.1  0.337  38       0\n",
      "..   ..  ...  ..  ..  ...   ...    ...  ..     ...\n",
      "738   2   99  60  17  160  36.6  0.453  21       0\n",
      "739   1  102  74   0    0  39.5  0.293  42       1\n",
      "740  11  120  80  37  150  42.3  0.785  48       1\n",
      "741   3  102  44  20   94  30.8  0.400  26       0\n",
      "742   1  109  58  18  116  28.5  0.219  22       0\n",
      "743   9  140  94   0    0  32.7  0.734  45       1\n",
      "744  13  153  88  37  140  40.6  1.174  39       0\n",
      "745  12  100  84  33  105  30.0  0.488  46       0\n",
      "746   1  147  94  41    0  49.3  0.358  27       1\n",
      "747   1   81  74  41   57  46.3  1.096  32       0\n",
      "748   3  187  70  22  200  36.4  0.408  36       1\n",
      "749   6  162  62   0    0  24.3  0.178  50       1\n",
      "750   4  136  70   0    0  31.2  1.182  22       1\n",
      "751   1  121  78  39   74  39.0  0.261  28       0\n",
      "752   3  108  62  24    0  26.0  0.223  25       0\n",
      "753   0  181  88  44  510  43.3  0.222  26       1\n",
      "754   8  154  78  32    0  32.4  0.443  45       1\n",
      "755   1  128  88  39  110  36.5  1.057  37       1\n",
      "756   7  137  90  41    0  32.0  0.391  39       0\n",
      "757   0  123  72   0    0  36.3  0.258  52       1\n",
      "758   1  106  76   0    0  37.5  0.197  26       0\n",
      "759   6  190  92   0    0  35.5  0.278  66       1\n",
      "760   2   88  58  26   16  28.4  0.766  22       0\n",
      "761   9  170  74  31    0  44.0  0.403  43       1\n",
      "762   9   89  62   0    0  22.5  0.142  33       0\n",
      "763  10  101  76  48  180  32.9  0.171  63       0\n",
      "764   2  122  70  27    0  36.8  0.340  27       0\n",
      "765   5  121  72  23  112  26.2  0.245  30       0\n",
      "766   1  126  60   0    0  30.1  0.349  47       1\n",
      "767   1   93  70  31    0  30.4  0.315  23       0\n",
      "\n",
      "[768 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df=pd.read_csv(\"C:/Users/wrm/Desktop/pima-indian-diabetes-analysis/pima-indians-diabetes.csv\",names=[\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"target\"])\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of the columns:\n",
    "\n",
    "0:Pregnancies\n",
    "\n",
    "1:Glucose\n",
    "\n",
    "2:BloodPressure\n",
    "\n",
    "3:SkinThickness\n",
    "\n",
    "4:Insulin\n",
    "\n",
    "5:BMI\n",
    "\n",
    "6:DiabetesPedigreeFunction\n",
    "\n",
    "7:Age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will select columns1->7, and change all the 0 to \"NaN.\" Because it is rediculous to use 0 to replace NaN in the modeling process. But it is really possible that a lady never pregnant.(There are possible wrong data here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0      1      2     3      4     5      6   7  target\n",
      "3     1   89.0   66.0  23.0   94.0  28.1  0.167  21       0\n",
      "4     0  137.0   40.0  35.0  168.0  43.1  2.288  33       1\n",
      "6     3   78.0   50.0  32.0   88.0  31.0  0.248  26       1\n",
      "8     2  197.0   70.0  45.0  543.0  30.5  0.158  53       1\n",
      "13    1  189.0   60.0  23.0  846.0  30.1  0.398  59       1\n",
      "14    5  166.0   72.0  19.0  175.0  25.8  0.587  51       1\n",
      "16    0  118.0   84.0  47.0  230.0  45.8  0.551  31       1\n",
      "18    1  103.0   30.0  38.0   83.0  43.3  0.183  33       0\n",
      "19    1  115.0   70.0  30.0   96.0  34.6  0.529  32       1\n",
      "20    3  126.0   88.0  41.0  235.0  39.3  0.704  27       0\n",
      "24   11  143.0   94.0  33.0  146.0  36.6  0.254  51       1\n",
      "25   10  125.0   70.0  26.0  115.0  31.1  0.205  41       1\n",
      "27    1   97.0   66.0  15.0  140.0  23.2  0.487  22       0\n",
      "28   13  145.0   82.0  19.0  110.0  22.2  0.245  57       0\n",
      "31    3  158.0   76.0  36.0  245.0  31.6  0.851  28       1\n",
      "32    3   88.0   58.0  11.0   54.0  24.8  0.267  22       0\n",
      "35    4  103.0   60.0  33.0  192.0  24.0  0.966  33       0\n",
      "39    4  111.0   72.0  47.0  207.0  37.1  1.390  56       1\n",
      "40    3  180.0   64.0  25.0   70.0  34.0  0.271  26       0\n",
      "43    9  171.0  110.0  24.0  240.0  45.4  0.721  54       1\n",
      "50    1  103.0   80.0  11.0   82.0  19.4  0.491  22       0\n",
      "51    1  101.0   50.0  15.0   36.0  24.2  0.526  26       0\n",
      "52    5   88.0   66.0  21.0   23.0  24.4  0.342  30       0\n",
      "53    8  176.0   90.0  34.0  300.0  33.7  0.467  58       1\n",
      "54    7  150.0   66.0  42.0  342.0  34.7  0.718  42       0\n",
      "56    7  187.0   68.0  39.0  304.0  37.7  0.254  41       1\n",
      "57    0  100.0   88.0  60.0  110.0  46.8  0.962  31       0\n",
      "59    0  105.0   64.0  41.0  142.0  41.5  0.173  22       0\n",
      "63    2  141.0   58.0  34.0  128.0  25.4  0.699  24       0\n",
      "68    1   95.0   66.0  13.0   38.0  19.6  0.334  25       0\n",
      "..   ..    ...    ...   ...    ...   ...    ...  ..     ...\n",
      "707   2  127.0   46.0  21.0  335.0  34.4  0.176  22       0\n",
      "709   2   93.0   64.0  32.0  160.0  38.0  0.674  23       1\n",
      "710   3  158.0   64.0  13.0  387.0  31.2  0.295  24       0\n",
      "711   5  126.0   78.0  27.0   22.0  29.6  0.439  40       0\n",
      "713   0  134.0   58.0  20.0  291.0  26.4  0.352  21       0\n",
      "715   7  187.0   50.0  33.0  392.0  33.9  0.826  34       1\n",
      "716   3  173.0   78.0  39.0  185.0  33.8  0.970  31       1\n",
      "718   1  108.0   60.0  46.0  178.0  35.5  0.415  24       0\n",
      "721   1  114.0   66.0  36.0  200.0  38.1  0.289  21       0\n",
      "722   1  149.0   68.0  29.0  127.0  29.3  0.349  42       1\n",
      "723   5  117.0   86.0  30.0  105.0  39.1  0.251  42       0\n",
      "726   1  116.0   78.0  29.0  180.0  36.1  0.496  25       0\n",
      "730   3  130.0   78.0  23.0   79.0  28.4  0.323  34       1\n",
      "732   2  174.0   88.0  37.0  120.0  44.5  0.646  24       1\n",
      "733   2  106.0   56.0  27.0  165.0  29.0  0.426  22       0\n",
      "736   0  126.0   86.0  27.0  120.0  27.4  0.515  21       0\n",
      "738   2   99.0   60.0  17.0  160.0  36.6  0.453  21       0\n",
      "740  11  120.0   80.0  37.0  150.0  42.3  0.785  48       1\n",
      "741   3  102.0   44.0  20.0   94.0  30.8  0.400  26       0\n",
      "742   1  109.0   58.0  18.0  116.0  28.5  0.219  22       0\n",
      "744  13  153.0   88.0  37.0  140.0  40.6  1.174  39       0\n",
      "745  12  100.0   84.0  33.0  105.0  30.0  0.488  46       0\n",
      "747   1   81.0   74.0  41.0   57.0  46.3  1.096  32       0\n",
      "748   3  187.0   70.0  22.0  200.0  36.4  0.408  36       1\n",
      "751   1  121.0   78.0  39.0   74.0  39.0  0.261  28       0\n",
      "753   0  181.0   88.0  44.0  510.0  43.3  0.222  26       1\n",
      "755   1  128.0   88.0  39.0  110.0  36.5  1.057  37       1\n",
      "760   2   88.0   58.0  26.0   16.0  28.4  0.766  22       0\n",
      "763  10  101.0   76.0  48.0  180.0  32.9  0.171  63       0\n",
      "765   5  121.0   72.0  23.0  112.0  26.2  0.245  30       0\n",
      "\n",
      "[392 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "df[\"1\"].replace(0,np.NaN,inplace=True)\n",
    "df[\"2\"].replace(0,np.NaN,inplace=True)\n",
    "df[\"3\"].replace(0,np.NaN,inplace=True)\n",
    "df[\"4\"].replace(0,np.NaN,inplace=True)\n",
    "df[\"5\"].replace(0,np.NaN,inplace=True)\n",
    "df[\"6\"].replace(0,np.NaN,inplace=True)\n",
    "df[\"7\"].replace(0,np.NaN,inplace=True)\n",
    "#print (df)\n",
    "df_new=df.dropna()\n",
    "print (df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7384615384615385\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X=df_new.drop(columns=[\"target\"])\n",
    "y=df_new[\"target\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial').fit(X_train, y_train)\n",
    "print (clf.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the result we find that there the prediction result is 0.7384. That is an excellent result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
